name: Nightly Code Quality

on:
  schedule:
    # Run at 3am UTC every day
    - cron: '0 3 * * *'
  workflow_dispatch: # Allow manual trigger

jobs:
  security-audit:
    name: Security Audit (pip-audit)
    runs-on: ubuntu-latest
    outputs:
      failed: ${{ steps.audit.outputs.failed }}
      details: ${{ steps.audit.outputs.details }}
    defaults:
      run:
        working-directory: backend

    steps:
      - uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: "latest"

      - name: Set up Python
        run: uv python install 3.11

      - name: Install dependencies
        run: uv sync

      - name: Run pip-audit (security scan)
        id: audit
        run: |
          echo "## Security Audit" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if uv run pip-audit 2>&1 | tee audit-output.txt; then
            echo "‚úÖ No known vulnerabilities found!" >> $GITHUB_STEP_SUMMARY
            echo "failed=false" >> $GITHUB_OUTPUT
          else
            echo "‚ö†Ô∏è Vulnerabilities detected:" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            cat audit-output.txt >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            echo "failed=true" >> $GITHUB_OUTPUT
            # Escape for JSON
            details=$(cat audit-output.txt | head -50 | jq -Rs .)
            echo "details=$details" >> $GITHUB_OUTPUT
            exit 1
          fi

  dead-code-analysis:
    name: Dead Code Analysis (Vulture)
    runs-on: ubuntu-latest
    outputs:
      failed: ${{ steps.vulture.outputs.failed }}
      details: ${{ steps.vulture.outputs.details }}
    defaults:
      run:
        working-directory: backend

    steps:
      - uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: "latest"

      - name: Set up Python
        run: uv python install 3.11

      - name: Install dependencies
        run: uv sync

      - name: Run Vulture (dead code detection)
        id: vulture
        run: |
          echo "## Dead Code Analysis" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          output=$(uv run vulture app/ --min-confidence 80 2>&1) || true

          if [ -z "$output" ]; then
            echo "‚úÖ No dead code detected!" >> $GITHUB_STEP_SUMMARY
            echo "failed=false" >> $GITHUB_OUTPUT
          else
            echo "‚ö†Ô∏è Potential dead code found:" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            echo "$output" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            echo "failed=true" >> $GITHUB_OUTPUT
            details=$(echo "$output" | head -50 | jq -Rs .)
            echo "details=$details" >> $GITHUB_OUTPUT
            exit 1
          fi

  code-quality:
    name: Code Quality Checks
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: backend

    steps:
      - uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: "latest"

      - name: Set up Python
        run: uv python install 3.11

      - name: Install dependencies
        run: uv sync

      - name: Run Ruff (linting)
        run: |
          echo "## Linting (Ruff)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if uv run ruff check . 2>&1 | tee ruff-output.txt; then
            echo "‚úÖ No linting issues!" >> $GITHUB_STEP_SUMMARY
          else
            echo "‚ö†Ô∏è Linting issues found:" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            cat ruff-output.txt >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          fi
        continue-on-error: true

      - name: Run MyPy (type checking)
        run: |
          echo "## Type Checking (MyPy)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if uv run mypy app --ignore-missing-imports 2>&1 | tee mypy-output.txt; then
            echo "‚úÖ No type errors!" >> $GITHUB_STEP_SUMMARY
          else
            echo "‚ö†Ô∏è Type issues found:" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            cat mypy-output.txt >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          fi
        continue-on-error: true

  coverage-check:
    name: Coverage Threshold Check
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: backend

    steps:
      - uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: "latest"

      - name: Set up Python
        run: uv python install 3.11

      - name: Install dependencies
        run: uv sync

      - name: Run tests with coverage
        run: |
          uv run pytest tests/ --cov=app --cov-report=term --cov-fail-under=85 --ignore=tests/e2e -q

          echo "## Coverage Report" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "‚úÖ Coverage meets 85% threshold" >> $GITHUB_STEP_SUMMARY

  chaos-tests:
    name: Chaos Tests (${{ matrix.browser }})
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        browser: [chromium, firefox, webkit]

    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: "latest"

      - name: Set up Python
        run: uv python install 3.11

      - name: Install backend dependencies
        working-directory: backend
        run: uv sync

      - name: Create data directory
        working-directory: backend
        run: mkdir -p data

      - name: Initialize database schema
        working-directory: backend
        run: uv run python -m scripts.init_db
        env:
          DATABASE_URL: "sqlite+aiosqlite:///./data/wallet.db"

      - name: Seed database
        working-directory: backend
        run: uv run python -m scripts.seed
        env:
          DATABASE_URL: "sqlite+aiosqlite:///./data/wallet.db"

      - name: Install frontend dependencies
        working-directory: frontend
        run: npm ci

      - name: Install Playwright browsers
        working-directory: frontend
        run: npx playwright install --with-deps ${{ matrix.browser }}

      - name: Run chaos tests
        working-directory: frontend
        run: npx playwright test --grep "@chaos" --project=${{ matrix.browser }}
        env:
          CI: true

      - name: Upload chaos test report
        uses: actions/upload-artifact@v4
        if: failure()
        with:
          name: chaos-report-${{ matrix.browser }}
          path: frontend/playwright-report/
          retention-days: 7

  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: "latest"

      - name: Set up Python
        run: uv python install 3.11

      - name: Install backend dependencies
        working-directory: backend
        run: uv sync

      - name: Run performance tests
        working-directory: backend
        run: |
          uv run pytest tests/performance -v -m performance --tb=short

          echo "## Performance Tests" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "‚úÖ Performance tests completed" >> $GITHUB_STEP_SUMMARY
        env:
          OTEL_ENABLED: "false"

      - name: Upload performance results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-results
          path: backend/test_perf.db
          retention-days: 7

  create-issues:
    name: Create Issues for Failures
    runs-on: ubuntu-latest
    needs: [security-audit, dead-code-analysis, code-quality, coverage-check, chaos-tests, performance-tests]
    if: failure()

    steps:
      - uses: actions/checkout@v4

      - name: Create issue for security vulnerabilities
        if: needs.security-audit.outputs.failed == 'true'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # Check if issue already exists
          existing=$(gh issue list --label "security" --state open --json number --jq 'length')
          if [ "$existing" -gt 0 ]; then
            echo "Security issue already exists, skipping"
            exit 0
          fi

          gh issue create \
            --title "üîí Security: Vulnerabilities detected in dependencies" \
            --label "security,automated" \
            --body "## Security Audit Failed

          The nightly security audit detected vulnerabilities in project dependencies.

          **Run:** [${{ github.run_id }}](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})

          ### Details
          \`\`\`
          ${{ needs.security-audit.outputs.details }}
          \`\`\`

          ### Next Steps
          1. Review the vulnerabilities above
          2. Update affected packages: \`uv lock --upgrade-package <package>\`
          3. If no fix available, assess risk and document in code"

      - name: Create issue for dead code
        if: needs.dead-code-analysis.outputs.failed == 'true'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # Check if issue already exists
          existing=$(gh issue list --label "dead-code" --state open --json number --jq 'length')
          if [ "$existing" -gt 0 ]; then
            echo "Dead code issue already exists, skipping"
            exit 0
          fi

          gh issue create \
            --title "üßπ Cleanup: Dead code detected" \
            --label "dead-code,automated,chore" \
            --body "## Dead Code Analysis Failed

          The nightly dead code analysis (Vulture) found potentially unused code.

          **Run:** [${{ github.run_id }}](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})

          ### Details
          \`\`\`
          ${{ needs.dead-code-analysis.outputs.details }}
          \`\`\`

          ### Next Steps
          1. Review each item - some may be false positives (e.g., used via reflection)
          2. Remove confirmed dead code
          3. Add to vulture whitelist if intentionally unused"

      - name: Create issue for coverage drop
        if: needs.coverage-check.result == 'failure'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # Check if issue already exists
          existing=$(gh issue list --label "coverage" --state open --json number --jq 'length')
          if [ "$existing" -gt 0 ]; then
            echo "Coverage issue already exists, skipping"
            exit 0
          fi

          gh issue create \
            --title "üìâ Coverage: Below 85% threshold" \
            --label "coverage,automated,testing" \
            --body "## Coverage Check Failed

          The nightly coverage check found that test coverage has dropped below the 85% threshold.

          **Run:** [${{ github.run_id }}](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})

          ### Next Steps
          1. Check the workflow run for detailed coverage report
          2. Identify untested code paths
          3. Add tests to restore coverage above 85%"

      - name: Create issue for chaos test failures
        if: needs.chaos-tests.result == 'failure'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # Check if issue already exists
          existing=$(gh issue list --label "chaos-testing" --state open --json number --jq 'length')
          if [ "$existing" -gt 0 ]; then
            echo "Chaos testing issue already exists, skipping"
            exit 0
          fi

          gh issue create \
            --title "üêí Chaos Testing: Failures detected" \
            --label "chaos-testing,automated,bug" \
            --body "## Chaos Tests Failed

          The nightly chaos/monkey tests found issues that may indicate UI instability.

          **Run:** [${{ github.run_id }}](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})

          ### Next Steps
          1. Download the Playwright report artifact from the failed run
          2. Review the seed values in failed tests to reproduce
          3. Fix identified crashes or error handling issues"

      - name: Create issue for performance regression
        if: needs.performance-tests.result == 'failure'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # Check if issue already exists
          existing=$(gh issue list --label "performance" --state open --json number --jq 'length')
          if [ "$existing" -gt 0 ]; then
            echo "Performance issue already exists, skipping"
            exit 0
          fi

          gh issue create \
            --title "üê¢ Performance: Test failures detected" \
            --label "performance,automated" \
            --body "## Performance Tests Failed

          The nightly performance tests detected failures or regressions.

          **Run:** [${{ github.run_id }}](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})

          ### Next Steps
          1. Review the performance test output in the workflow run
          2. Check for recent changes that may have impacted performance
          3. Profile affected endpoints to identify bottlenecks"
